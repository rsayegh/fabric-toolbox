{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf2f893-4330-42db-a8ec-273ea7c1b2d4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Install the latest .whl package\n",
    "\n",
    "Check [here](https://pypi.org/project/semantic-link-labs/) to see the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0510e2c-5104-400e-88e2-37b18067d618",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5372acf8-451a-4baf-9c40-a80c829fb358",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Import the library and set initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e5bbc-5152-496b-bc98-1bfb88f9b5f1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "from sempy_labs import migration, directlake\n",
    "import sempy_labs.report as rep\n",
    "import time\n",
    "\n",
    "datamart_workspace_name        = ''             # Specify the name of the workspace containing the datamart semantic model.\n",
    "datamart_dataset_name          = ''             # Specify the name of the datamart semantic model.\n",
    "semantic_model_workspace_name  = ''             # Specify the name of the workspace containing the new semantic model will be stored. This can be the same as the datamart workspace.\n",
    "semantic_model_name            = ''             # Specify the name of the new Direct Lake semantic model.\n",
    "item_workspace_name            = ''             # Specify the name of the workspace containing the data warehouse that the datamart was migrated to.\n",
    "item_name                      = ''             # Specify the name of the warehouse that the datamart was migrated to.\n",
    "schema_name                    = ''             # Specify the name of the schema where the data is stored in the data warehouse.\n",
    "item_type                      = 'Warehouse'    # Specify the item type for the migration; options are 'Warehouse' or 'Lakehouse'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a0eb4-9e97-470a-8af4-84a3eea16abc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Function: v2_migrate_tables_columns_to_semantic_model\n",
    "\n",
    "Includes support of multiple item types in Fabric including the options: Lakehouse and Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec2f2c-63e9-441a-b250-68a5be5d551c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sempy.fabric as fabric\n",
    "from sempy_labs._helper_functions import resolve_lakehouse_name, retry\n",
    "from sempy_labs.directlake import generate_shared_expression\n",
    "from sempy_labs.lakehouse._lakehouse import lakehouse_attached\n",
    "from sempy_labs.tom import connect_semantic_model\n",
    "from typing import Optional\n",
    "from sempy._utils._log import log\n",
    "import sempy_labs._icons as icons\n",
    "\n",
    "@log\n",
    "def v2_migrate_tables_columns_to_semantic_model(\n",
    "    dataset: str,\n",
    "    new_dataset: str,\n",
    "    workspace: Optional[str] = None,\n",
    "    new_dataset_workspace: Optional[str] = None,\n",
    "    item_name: Optional[str] = None,\n",
    "    item_workspace: Optional[str] = None,\n",
    "    item_type: Optional[str] = 'Lakehouse',\n",
    "    schema_name: Optional[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Adds tables/columns to the new Direct Lake semantic model based on an import/DirectQuery semantic model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "        Name of the import/DirectQuery semantic model.\n",
    "\n",
    "    new_dataset : str\n",
    "        Name of the Direct Lake semantic model.\n",
    "\n",
    "    workspace : str, default=None\n",
    "        The Fabric workspace name in which the import/DirectQuery semantic model exists.\n",
    "        Defaults to None which resolves to the workspace of the attached lakehouse\n",
    "        or if no lakehouse attached, resolves to the workspace of the notebook.\n",
    "\n",
    "    new_dataset_workspace : str\n",
    "        The Fabric workspace name in which the Direct Lake semantic model will be created.\n",
    "        Defaults to None which resolves to the workspace of the attached lakehouse\n",
    "        or if no lakehouse attached, resolves to the workspace of the notebook.\n",
    "\n",
    "    item_name : str, default=None\n",
    "        The item name used by the Direct Lake semantic model.\n",
    "        Defaults to None which resolves to the lakehouse attached to the notebook.\n",
    "\n",
    "    item_workspace : str, default=None\n",
    "        The Fabric workspace used by the item.\n",
    "        Defaults to None which resolves to the workspace of the attached lakehouse\n",
    "        or if no lakehouse attached, resolves to the workspace of the notebook.\n",
    "\n",
    "    item_type : str, default=lakehouse\n",
    "        The item type that the Direct Lake semantic model will be created from.\n",
    "        Defaults to lakehouse, supported items include [Lakehouse, Warehouse].\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == new_dataset:\n",
    "        raise ValueError(\n",
    "            f\"{icons.red_dot} The 'dataset' and 'new_dataset' parameters are both set to '{dataset}'. These parameters must be set to different values.\"\n",
    "        )\n",
    "\n",
    "    # Array for supported item types\n",
    "    item_types = [\"Lakehouse\", \"Warehouse\"]\n",
    "    item_type = item_type.capitalize()\n",
    "    if item_type not in item_types:\n",
    "        raise ValueError(\n",
    "            f\"{icons.red_dot} Invalid item type. Valid options: {item_types}.\"\n",
    "        )\n",
    "\n",
    "    workspace = fabric.resolve_workspace_name(workspace)\n",
    "\n",
    "    if new_dataset_workspace is None:\n",
    "        new_dataset_workspace = workspace\n",
    "\n",
    "    if item_workspace is None:\n",
    "        item_workspace = new_dataset_workspace\n",
    "\n",
    "    shEx = generate_shared_expression(item_name, item_type, item_workspace)\n",
    "\n",
    "    dfC = fabric.list_columns(dataset=dataset, workspace=workspace)\n",
    "    dfT = fabric.list_tables(dataset=dataset, workspace=workspace)\n",
    "    dfT.rename(columns={\"Type\": \"Table Type\"}, inplace=True)\n",
    "    dfC = pd.merge(\n",
    "        dfC,\n",
    "        dfT[[\"Name\", \"Table Type\"]],\n",
    "        left_on=\"Table Name\",\n",
    "        right_on=\"Name\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    dfT_filt = dfT[dfT[\"Table Type\"] == \"Table\"]\n",
    "    dfC_filt = dfC[\n",
    "        (dfC[\"Table Type\"] == \"Table\")\n",
    "        & ~(dfC[\"Column Name\"].str.startswith(\"RowNumber-\"))\n",
    "        & (dfC[\"Type\"] != \"Calculated\")\n",
    "    ]\n",
    "\n",
    "    print(f\"{icons.in_progress} Updating '{new_dataset}' based on '{dataset}'...\")\n",
    "\n",
    "    @retry(\n",
    "        sleep_time=1,\n",
    "        timeout_error_message=f\"{icons.red_dot} Function timed out after 1 minute\",\n",
    "    )\n",
    "    def dyn_connect():\n",
    "        with connect_semantic_model(\n",
    "            dataset=new_dataset, readonly=True, workspace=new_dataset_workspace\n",
    "        ) as tom:\n",
    "\n",
    "            tom.model\n",
    "\n",
    "    dyn_connect()\n",
    "\n",
    "    with connect_semantic_model(\n",
    "        dataset=new_dataset, readonly=False, workspace=new_dataset_workspace\n",
    "    ) as tom:\n",
    "        if not any(e.Name == \"DatabaseQuery\" for e in tom.model.Expressions):\n",
    "            tom.add_expression(\"DatabaseQuery\", expression=shEx)\n",
    "            print(f\"{icons.green_dot} The 'DatabaseQuery' expression has been added.\")\n",
    "\n",
    "        for i, r in dfT_filt.iterrows():\n",
    "            tName = r[\"Name\"]\n",
    "            tDC = r[\"Data Category\"]\n",
    "            tHid = bool(r[\"Hidden\"])\n",
    "            tDesc = r[\"Description\"]\n",
    "            ent_name = tName\n",
    "            for char in icons.special_characters:\n",
    "                ent_name = ent_name.replace(char, \"\")\n",
    "            if schema_name != None:\n",
    "                tSLT = f\"[{schema_name}].[{tName}]\"\n",
    "            if not any(t.Name == tName for t in tom.model.Tables):\n",
    "                tom.add_table(\n",
    "                    name=tName,\n",
    "                    description=tDesc,\n",
    "                    data_category=tDC,\n",
    "                    hidden=tHid,\n",
    "                    source_lineage_tag=tSLT\n",
    "                )\n",
    "                tom.add_entity_partition(table_name=tName, entity_name=ent_name, schema_name=schema_name)\n",
    "                print(f\"{icons.green_dot} The '{tName}' table has been added.\")\n",
    "\n",
    "        for i, r in dfC_filt.iterrows():\n",
    "            tName = r[\"Table Name\"]\n",
    "            cName = r[\"Column Name\"]\n",
    "            scName = r[\"Source\"]\n",
    "            cHid = bool(r[\"Hidden\"])\n",
    "            cDataType = r[\"Data Type\"]\n",
    "            for char in icons.special_characters:\n",
    "                scName = scName.replace(char, \"\")\n",
    "\n",
    "            if scName.endswith(\"_\"):\n",
    "                scName = scName[:-1]\n",
    "\n",
    "            if not any(\n",
    "                c.Name == cName and c.Parent.Name == tName for c in tom.all_columns()\n",
    "            ):\n",
    "                tom.add_data_column(\n",
    "                    table_name=tName,\n",
    "                    column_name=cName,\n",
    "                    source_column=scName,\n",
    "                    hidden=cHid,\n",
    "                    data_type=cDataType,\n",
    "                )\n",
    "                print(\n",
    "                    f\"{icons.green_dot} The '{tName}'[{cName}] column has been added.\"\n",
    "                )\n",
    "\n",
    "        print(\n",
    "            f\"\\n{icons.green_dot} All regular tables and columns have been added to the '{new_dataset}' semantic model.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b86947-68e7-424d-9202-d660e190013e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create the Direct Lake model based on the datamart semantic model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb42958-7d53-4591-9d07-5015563c7ffa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "labs.create_blank_semantic_model(dataset = semantic_model_name, workspace = semantic_model_workspace_name)\n",
    "\n",
    "v2_migrate_tables_columns_to_semantic_model(\n",
    "    workspace               = datamart_workspace_name,\n",
    "    dataset                 = datamart_dataset_name,\n",
    "    new_dataset_workspace   = semantic_model_workspace_name,\n",
    "    new_dataset             = semantic_model_name,\n",
    "    item_workspace          = item_workspace_name,\n",
    "    item_name               = item_name,\n",
    "    item_type               = item_type,\n",
    "    schema_name             = schema_name\n",
    ")\n",
    "\n",
    "migration.migrate_model_objects_to_semantic_model(\n",
    "    workspace               = datamart_workspace_name,\n",
    "    dataset                 = datamart_dataset_name,\n",
    "    new_dataset_workspace   = semantic_model_workspace_name,\n",
    "    new_dataset             = semantic_model_name\n",
    ")\n",
    "\n",
    "migration.migrate_field_parameters(\n",
    "    workspace               = datamart_workspace_name,\n",
    "    dataset                 = datamart_dataset_name,\n",
    "    new_dataset_workspace   = semantic_model_workspace_name,\n",
    "    new_dataset             = semantic_model_name\n",
    ")\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "labs.refresh_semantic_model(workspace = semantic_model_workspace_name, dataset = semantic_model_name)\n",
    "\n",
    "migration.refresh_calc_tables(workspace = semantic_model_workspace_name, dataset = semantic_model_name)\n",
    "\n",
    "labs.refresh_semantic_model(workspace = semantic_model_workspace_name, dataset = semantic_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdedabe-df97-4750-b54f-c5bbacebbdc6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Show migrated/unmigrated objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd47aa-0c0e-4a21-a64c-becac99871ba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "migration.migration_validation(\n",
    "    workspace               = datamart_workspace_name, \n",
    "    dataset                 = datamart_dataset_name,\n",
    "    new_dataset_workspace   = semantic_model_workspace_name,\n",
    "    new_dataset             = semantic_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb30dfc-8603-47a8-b0bc-89faf3832b40",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Rebind all reports using the old semantic model to the new Direct Lake semantic model\n",
    "\n",
    "**IMPORTANT:** The data is not imported from the original datamart to the data warehouse. It is recommended to evaluate available data ingestion options, such as Dataflow Gen2, data pipelines, Copy Jobs, and more, to determine the optimal choice for your solution before rebinding existing reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485ff58-9d4f-4683-85ba-656b466f94f5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "rep.report_rebind_all(\n",
    "    dataset_workspace     = datamart_workspace_name,\n",
    "    dataset               = datamart_dataset_name,\n",
    "    new_dataset_workpace  = semantic_model_workspace_name,\n",
    "    new_dataset           = semantic_model_name,\n",
    "    report_workspace      = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ea1f9-d900-4b46-80dc-6c7897cd7b29",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Rebind reports one-by-one (optional)\n",
    "\n",
    "**IMPORTANT:** The data is not imported from the original datamart to the data warehouse. It is recommended to evaluate available data ingestion options, such as Dataflow Gen2, Data pipelines, Copy jobs, and more, to determine the optimal choice for your solution before rebinding existing reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ea9db-5236-4baf-aad9-fc92b7385687",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "report_workspace_name   = 'Datamart Migration' # Enter workspace where the report which you want to rebind is stored\n",
    "report_name             = 'Sales Analysis'     # Enter report name which you want to rebind to the new Direct Lake model\n",
    "\n",
    "rep.report_rebind(\n",
    "    report_workspace    = report_workspace_name,\n",
    "    report              = report_name,\n",
    "    dataset_workspace   = semantic_model_workspace_name,\n",
    "    dataset             = semantic_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c078da3-137c-4661-a2b5-41b516bb1156",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Show unsupported objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89d746-4fbe-4920-a6f9-75f2fecb1fc2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dfT, dfC, dfR = directlake.show_unsupported_direct_lake_objects(dataset = datamart_dataset_name, workspace = datamart_workspace_name)\n",
    "\n",
    "print('Calculated Tables are not supported...')\n",
    "display(dfT)\n",
    "print(\"Learn more about Direct Lake limitations here: https://learn.microsoft.com/power-bi/enterprise/directlake-overview#known-issues-and-limitations\")\n",
    "print('Calculated columns are not supported. Columns of binary data type are not supported.')\n",
    "display(dfC)\n",
    "print('Columns used for relationship must be of the same data type.')\n",
    "display(dfR)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "9fa54561-2905-41ff-83ca-099e54ff93ea",
    "default_lakehouse_name": "DataflowsStagingLakehouse",
    "default_lakehouse_workspace_id": "27bf929d-87c4-4bb9-a39a-2c9a49fdaa70"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
